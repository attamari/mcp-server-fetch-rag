[project]
name = "mcp-server-fetch-rag"
version = "0.2.0"
description = "A Model Context Protocol server providing RAG-enhanced web fetching for LLMs"
readme = "README.md"
requires-python = ">=3.11,<3.14"
license = { text = "MIT" }
keywords = ["mcp", "llm", "rag", "web", "fetch"]
dependencies = [
    "httpx==0.28.1",
    "mcp==1.26.0",
    "protego==0.5.0",
    "pydantic==2.12.5",
    "trafilatura==2.0.0",
    "fastembed==0.7.4",
    "wtpsplit-lite==0.2.0",
    "numpy==2.4.1",
    "pypdfium2==5.3.0",
    # Platform-specific ONNX Runtime with GPU acceleration
    "onnxruntime-directml==1.23.0; sys_platform == 'win32'",  # Windows: DirectML (AMD/Intel/NVIDIA)
    "onnxruntime-gpu==1.23.2; sys_platform == 'linux'",       # Linux: CUDA (fallback to CPU if no GPU)
]

[project.optional-dependencies]
# GPU acceleration options (install one based on your hardware)
cuda = ["onnxruntime-gpu==1.23.2"]  # NVIDIA GPU (CUDA 11.x)
cuda12 = ["onnxruntime-gpu==1.23.2"]  # NVIDIA GPU (CUDA 12.x) - install via: pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/
directml = ["onnxruntime-directml==1.23.0"]  # Windows GPU (AMD/Intel/NVIDIA via DirectX)
openvino = ["onnxruntime-openvino==1.23.0"]  # Intel CPU/GPU optimization

[project.scripts]
mcp-server-fetch-rag = "mcp_server_fetch_rag:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.sdist]
include = ["src/mcp_server_fetch_rag"]

[tool.hatch.build.targets.wheel]
packages = ["src/mcp_server_fetch_rag"]

[tool.uv]
dev-dependencies = [
    "pyright==1.1.408",
    "ruff==0.14.14",
]
